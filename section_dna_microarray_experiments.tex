Scientific inquiries that use microarray technology to address research
questions are sometimes called \emph{microarray experiments}.  Experiments can
be divided into two general classes, pattern detection and predictive modeling.
These are described in greater detail in Sections \ref{Pattern
Detection}-\ref{Predictive Modeling}.  The method descriptions given here are
desribed in terms of the characterization of biological samples, but it is
important to bear in mind that the samples $x$ genes data matrix can be
transposed and that these same methods can be used for the characterization of
genes across multiple phenotypic conditions.  Indeed, this is also an area
warranting further exploration given that gene-centric analyses are not stymied
by the ``curse of dimensionality'', discussed in greater detail in Section
\ref{Feature Selection}.

\subsection{Pattern Detection}
\label{Pattern Detection}

The first class of experiment is data-driven, in which an \emph{a-priori}
hypothesis is not explictly stated and sample metadata in the form of labels
are not provided.  This data driven method is sometimes also called
\emph{pattern detection} because experiments employing it focus mainly on
identifying biologically interesting and previously unobserved correlations
between factors of the experiment \cite{Dubitzky2003IMD}.  A common type of
pattern detection experiment seen in the literature examines biological samples
that are indistinguishable using lower-resolution assay technologies to see if
the samples can be broken into sub-groups by collecting more data using the
higher-resolution microarray.   A study by Freije, \emph{et al.} used this
approach to identify novel subclasses of malignant gliomas
\cite{PMID_15374961}.

\subsection{Predictive Modeling}
\label{Predictive Modeling}

In the second general class of experiment seen in the literature, the emphasis
is on constructing a function that is able to \emph{classify}, or predict class
labels for unlabeled samples.  This method is sometimes called \emph{supervised
learning} or \emph{predictive modeling} \cite{Dubitzky2003IMD}.  A predictive
modeling study begins with a group of samples for which assay data as well as
class labels are available.  This initial set of labeled samples is divided
into two sub-groups, \emph{training data} and \emph{test data}.  The
training data are processed using a classification algorithm to build a model
of the relationship between class label and assay data.  One major area of
concern in the construction of the model and the application of that model to
unlabeled samples is the number of features considered.  Feature selection is
discussed in greater detail in Section \ref{Feature Selection}.

The performance of the classification model produced using the training data is
typically assessed and iteratively optimized to minimize error using $K$-way
cross validation for all class labels $K$.  However, in some cases class labels
$K$ are nested meaning that the class $K_{i+1}$ can be a refinement of the
class $K_{i}$.  In these hierarchical classification models, optimization and
error minimization consider the graph structure \cite{pachinko}.  Once a
satisfactorily low error rate has been achieved, the predictive power of the
classification model can be estimated by using it to predict labels for the
test data that were set aside prior to model building.  A study by Day and Dong
in Chapter \classchapter demonstrates the usage of both flat and hierarchical
classification methods.

\subsection{Feature Selection}
\label{Feature Selection}

A prominent part of model building for the analysis of microarray data is
\emph{feature selection}, or the process of identifying which features of
observations are relevant in the assignment of class labels..  Selected
features correspond to the microarray feature measurements, or some
summarization thereof (Section \ref{Summarization}).  Feature selection is
essential because the analytical methods applied to microarray experiments bear
the ``curse of dimensionality''.  In a typical experiment the number of samples
is small ($1x10^1 \dots 1x10^3$), while the number of measurements made on each
sample is relatively large ($1x10^3 \dots 1x10^6$) \cite{Bellman_1957}.
Because of this it is possible to correctly partition labeled data on one or
multiple subsets of all observed features.  Using a subset that robustly
captures the sample labels is desireable because it increases classification
accuracy and reduces the computation required required to calculate a new
sample label, as many classification algorithms scale exponentially with the
number of features considered \cite{john94irrelevant}.

\subsection{Label Encoding}
\label{Label Encoding}

introduce the problem of the richness of gene annotation, gene ontology,
hierarchical labels, label dimensions, etc.  this problem is in its infancy for
the labeling of samples now, but will ultimately be far more complex than gene
annotation which is currently more mature.
