Scientific inquiries that use microarray technology to address research
questions are referred to as \emph{microarray experiments}.  The majority of
these experiments can be divided into two classes: pattern detection and
predictive modeling.  These are described in greater detail in Sections
\ref{Pattern Detection}-\ref{Predictive Modeling}, and specific examples of
these experimental classes can be seen in \cite{Dudoit2003ICM, Azuaje2003cge,
Stanford2003cac, PMID_12413821, PMID_15843092, PMID_15867208}.  The method
descriptions given here are desribed in terms of the characterization of
biological samples, but it is important to bear in mind that the
samples-by-genes data matrices can be transposed and that these same methods
can be used for the characterization of genes across multiple phenotypic
conditions.  Indeed, this is also an area warranting further exploration given
that gene-centric analyses are not stymied by the ``curse of dimensionality'',
discussed in greater detail in Section \ref{Feature Selection}.

\subsection{Pattern Detection}\label{Pattern Detection}

The first class of experiment is data-driven, in which an \emph{a-priori}
hypothesis is not explictly stated and sample metadata in the form of labels
are not provided.  This data driven method is sometimes also called
\emph{pattern detection} because experiments employing it focus mainly on
identifying biologically interesting and previously unobserved correlations
between factors of the experiment \cite{Dubitzky2003IMD}.  A common type of
pattern detection experiment seen in the literature examines biological samples
that are indistinguishable using lower-resolution assay technologies to see if
the samples can be broken into sub-groups by collecting more data using the
higher-resolution microarray.   A study by Freije, \emph{et al.} used this
approach to identify novel subclasses of malignant gliomas
\cite{PMID_15374961}.

\subsection{Predictive Modeling}\label{Predictive Modeling}

In the second general class of experiment seen in the literature, the emphasis
is on constructing a function that is able to \emph{classify}, or predict class
labels for unlabeled samples.  This method is sometimes called \emph{supervised
learning} or \emph{predictive modeling} \cite{Dubitzky2003IMD}.  A predictive
modeling study begins with a group of samples for which assay data as well as
class labels are available.  This initial set of labeled samples is divided
into two sub-groups, \emph{training data} and \emph{test data}.  The
training data are processed using a classification algorithm to build a model
of the relationship between class label and assay data.  One major area of
concern in the construction of the model and the application of that model to
unlabeled samples is the number of features considered.  Feature selection is
discussed in greater detail in Section \ref{Feature Selection}.

The performance of the classification model produced using the training data is
typically assessed and iteratively optimized to minimize error using $K$-way
cross validation for all class labels $K$.  However, in some cases class labels
$K$ are nested meaning that the class $k_{i}$ can be a refinement of the
class $k_{i-1}$.  In these hierarchical classification models, optimization and
error minimization consider the graph structure \cite{pachinko}.  Once a
satisfactorily low error rate has been achieved, the predictive power of the
classification model can be estimated by using it to predict labels for the
test data that were set aside prior to model building.
% A study by Day and Dong in Chapter \classchapter demonstrates the usage of
% both flat and hierarchical classification methods.

\subsection{Feature Selection}\label{Feature Selection}

A prominent part of model building for the analysis of microarray data is
\emph{feature selection}, or the process of identifying which features of
observations are relevant in the assignment of class labels.  Selected features
correspond to the microarray feature measurements, or some summarization
thereof (Section \ref{Summarization}).

Feature selection has been essential because the analytical methods applied to
microarray experiments bear the ``curse of dimensionality''
\cite{Bellman_1957}.  In a typical experiment the number of samples is small
($1{\times}10^1 \dots 1{\times}10^3$), while the number of measurements made on
each sample is relatively large ($1{\times}10^3 \dots 1{\times}10^6$).  Because
of this it is possible to correctly partition labeled data on one or multiple
subsets of all observed features.  Using a subset that robustly captures the
sample labels is desireable because it increases classification accuracy and
reduces the computation required required to calculate a new sample label, as
many classification algorithms scale exponentially with the number of features
considered \cite{john94irrelevant}.

However, the work described in Chapter \celsiuschapter presents a mechanism for
aggregation and analysis of much larger numbers of samples than are commonplace
in experiments to date.

\subsection{Label Encoding}\label{Label Encoding}

In both the pattern detection (Section \ref{Pattern Detection}) and predictive
modeling (Section \ref{Predictive Modeling}) classes of microarray studies, a
class label from one or more multiple orthogonal experimental factors is
attached to a sample.  Class labels are typically ``flat'', meaning that there
is no hierarchical structure and that the distance between any two classes is
uniform.  However, It is well-established that such a flat representation is
not suitable for representing gene annotation \cite{go}, and it has been
demonstrated that statistical analyses that consider the structure of
relationships between annotations bear more power than their flat counterparts
\cite{ease,gocluster,pachinko}.  Thus, analyses that explicitly model the
relationship types and distances between class labels of samples are expected
to become more commonplace as open, community-supported, standard structures
for encoding sample annotations mature \cite{so, obo, mpath, cl, mo, ma, mpath,
mp}.  Encoding metadata using open standards also has implications with regard
to integration of results from multiple studies, or otherwise exchanging data.
These aspects are discussed in Section \ref{Protocol}.
