%%% XXX this belongs in a higher-level or dedicated section.
%To get around this problem of co-processing, Katz \emph{et al.} described a
%method for large scale, iterative processesing of Affymetrix microarrays
%\cite{genelogic}.  Because multi-array methods for performing background correction

%ok
Microarray pre-processing, also known as \emph{quantification}, is the process
of estimating the quantity of each gene in the sample that was assayed in the
hybridization step (Section \ref{Hybridization}) of the experiment.
Quantification can be broken down into four distinct sub-procedures, executed
in the following order: image processing, background correction, normalization,
and summarization.  Much of the specific details in this section are
idiosyncratic to the Affymetrix GeneChip platform, but general principles apply
to all forms of microarray technology.

%ok
\subsection{Image Processing}\label{Image Processing}

The term \emph{image processing} is used to describe a set of steps that
transforms the physical microarray into a digital file suitable for subsequent
processing by a computer.  The techical details of the protocol vary, but the
general principles described here remain the same.

As discussed in Section \ref{Hybridization}, fluorescent dyes are linked to the
synthesized DNA sequences that are hybridized to the microarray.  The
microarray is placed in a digital scanning apparatus that contains laser(s)
that can emit light at the excitatory frequency of each dye.  The apparatus
also contains photosensors that are able to detect the fluorescent light from
the dye(s).  Each laser, then, is scanned across the surface of the microarray
and the photosensors record the position and intensity of light for each dye.
The raw form of these data is typically a series of images encoded using a
lossless format, such as TIFF.  The image is then processed by an alignment
algorithm that aligns the captured image to the coordinate system known to have
been printed/synthesized onto the microarray surface.

Finally, platform-specific protocols are used to represent the raw image in a
format more suitable for downstream processing.  For example, with
single-channel Affymetrix microarrays, the TIFF image is converted to an
alternate format, called \emph{CEL} format, that describes the attributes of
each microaray feature, such as mean intensity, variance, and number of pixels
that represent the feature.

%ok
\subsection{Background Correction}\label{Background Correction}

Background correction is a statistical procedure that estimates and removes low
levels of noise on the microarray.  Background noise can have many sources.

The simplest and most common source of background noise is optical.  It can be
caused by general cross-hybridization of target to all probes, mis-calibration
of the microarray scanner's photo-sensor, and diffused or reflected light from
the laser used to excite the fluorescent dyes.  Optical noise can be estimated
by measuring the level of fluorescence from featureless regions of the
microarray and negative control probes that are not reverse-complementary to
any sequences in the hybridization mixture.  These measure background-level
reflected light and the level of non-specific hybridization, respectively.

Manufacturing and hybridization artifacts, such as surface scratches and salt
residues, are another source of noise.  A simple form of location-based
background correction is descibed in the Statistical Algorithms Description
Document \cite{affy:tech:2002}.   Briefly, the chip is broken into a $4x4$
grid of 16 rectangular regions.  The lowest 2\% of each region's probe
intensities are used to compute a background value for that region.  Each probe
(PM and MM) is then adjusted based upon a weighted average of the backgrounds
for all regions. The weights are based on the distances between the location of
the probe and the centroids of all regions.  More sophisticated methods attempt
to detect areas of the microarray containing high levels of manufacturing and
hybridization noise.  Noisy areas can be identified because the probes located
there will be outliers relative to probes for the same target located elsewhere
on the microarray.  Probes in these areas are considered unreliable and are
either given a very low weight parameter or are removed from normalization
(Section \ref{Normalization}) and other downstream processing (Section
\ref{Summarization}) altogether \cite{affyplm}.

Newer, multi-array background correction methods have leveraged existing data
to build a models of how background noise is generally distributed.  The gcRMA
model \cite{gcrma} includes a parameter the sequence composition of each probe,
while other models such as those used in the RMA and MBEI (dChip)
\cite{rma,mbei,dchip,vsn,bioc} methods only include a parameter for concordant
each probe is with other probes in the same set.  The RMA background correction
method is the \i{de facto} standard, and corrects perfect match (PM) probe
intensities by using a global model for the distribution of probe intensities
\cite{rma,gautier}.

%The model is suggested by looking at plots of the empirical distribution of
%probe intensities.  In particular the observed PM probes are modeled as the sum
%of a normal noise component N (Normal with mean $\mu$ and variance $\sigma^2$)
%and a exponential signal component S (exponential with mean $\alpha$). To avoid
%any possibility of negatives, the normal is truncated at zero. Given we have O
%the observed intensity, this then leads to an adjustment, given in Equation
%\ref{rmabg}:
%
%\begin{equation}
%\label{rmabg}
%E\left(s \lvert O=o\right) = a + b \frac{\phi\left(\frac{a}{b}\right) - \phi\left(\frac{o-a}{b}\right)}{\Phi\left(\frac{a}{b}\right) + \Phi\left(\frac{o-a}{b}\right) - 1 }
%\end{equation}
%
%where $a =  s- \mu - \sigma^2\alpha$ and $b = \sigma$. Note that $\phi$ and
%$\Phi$ are the standard normal distribution density and distribution functions
%respectively.  Note also that MM probe intensities are not corrected by either
%of these routines \cite{rma,bioc}.

Multi-array background correction methods are able to detect background noise
due to the manufacturing and hybridization artifacts described above, but the
size of the aray artifact can be as small as a single feature.  This should in
principle do a better job of noise estimation.  A major drawback to multi-array
background noise models is that the noise estimates are only valid in the
context of the co-processed set of microarrays.  This is because the noise
estimates are derived from parameter estimates specific to that set of
microarrays.  While this is not a problem for small-scale analysis on
individual experiments, it creates difficulties when merging data from multiple
experiments because all microarrays will need to be re-processed to re-fit the
parameters of the noise model.  This re-processing problem can become
intractable for background correcting a very large number of arrays.  Further
attention is given to tractability in Section \ref{Scalability}.

%ok
\subsection{Normalization}\label{Normalization}

After correcting for background noise (Section \ref{Background Correction}),
microarrays are normalized.  The purpose of normalization is to transform the
distribution of microarray measurements so that properties of the distribution
of measurements match expectations (e.g., a log-normal distribution).

The most simple form of microarray normalization is a linear scaling.  The
Affymetrix MAS 5 algorithm \cite{mas5} performs linear scaling by (1) setting
aside the top and bottom 1\% of measurements as outliers, adjusts the mean of
the remaining measurements to a constant value, then multiplies each
measurement, including the outliers, by the factor used to adjust the mean.

The normalization method used in the dChip software \cite{mbei,dchip} selects a
baseline microarray.   Then, all microarrays are normalized by selecting
invariant sets of probes within each of the ``treatment'' and ``baseline''
conditions.  These are used to fit a non-linear relationship between the two
conditions, and this relationship is used to carry out the normalization.

Many other normalization methods exist.  The essentially differ in two aspects:
the theoretical model of how the microarray behaves, and the techniques used to
fit the observed data to that model.  In this regard, normalization techniques
are similar to those used for background correction (Section \ref{Background
Correction}) and summarization (Section \ref{Summarization}).

A comparison of the performance of a larg number of normalization methods at
correctly estimating RNA concentration on a standard, synthetic data sets
published by Affymetrix is described in \cite{affybench}.

\subsection{Summarization}
\label{Summarization}

The last step in microarray data preprocessing is to combine the measurements
from all probes in a probe set into a single value.  This procedure is called
\emph{summarization}.

The simplest summarization algorithm, called ``average difference''
\cite{affy4} computes the mean of difference between each PM/MM probe pair
(Equation \ref{avgdiffsummary}),

\begin{equation}
\label{avgdiffsummary}
y_{k} = I_i^{-1}{\sum_{i=1}^{I_k} |PM_i-MM_i|}
\end{equation}

where the probe set $k$ has $PM$ perfect match and $MM$ mismatch probe pairs $i
= 1,\dots,I_k$.

Summarization methods parallel background correction and normalization methods
in that there are two varieties, the single-array methods and the multi-array
methods.  ``Average difference'' is an example of the former.  Multi-array
methods consider the distribution of probe measurements across all arrays, and
in some cases assign an array-specific parameter used to compute the probe set
summary.  The summarization component of the MBEI method introduced by Li and
Wong \cite{mbei,dchip} is given in Equation \ref{mbeisummary},

\begin{equation}
\label{mbeisummary}
y_{ij} = \phi_i \theta_j + \epsilon_{ij}
\end{equation}

where $y_{ij}$ is $PM_{ij}$ or the difference between $PM_{ij}-MM_{ij}$. The
$\phi_i$ parameter is a probe response parameter and $\theta_j$ is the
expression on microarray $j$.

The summarization component of RMA pre-processing \cite{rma} performss a
multi-array linear fit to data from each probe set.  Specifically, for probe
set $k$ with $i=1,\dots,I_k$ probe pairs and microarrays $j=1,\dots,J$ the
model given in Equation \ref{medianpolish} is fit,

\begin{equation}
\label{medianpolish}
\log_2\left(PM^{(k)}_{ij}\right) = \alpha_i^{(k)} + \beta_j^{(k)} + \epsilon_{ij}^{(k)}
\end{equation}

where $\alpha_i$ is a probe effect and $\beta_j$ is the $\log_2$ expression
value, and the method is known as \emph{median polish}, named after Tukey's
algorithm used to perform the calculation.

It is noteworthy that summarized probe set values from all popular multi-array
summarization methods, including those described here, are dependent upon the
probe set and microarray effect parameters calculated as part of the model fit.
While this is not a problem theoritically, it introduces unique challenges in
the implementation of a pre-processing pipeline for a large number of arrays.
This is discussed in greater detail in Section \ref{Scalability}.

\subsection{Scalability}\label{Scalability}

While the superiority of multi-array techniques described in Sections
\ref{Background Correction}-\ref{Summarization} has been established, a major
impediment to implementing these in practice is that they are not designed to
scale to large data sets due to the amount of system resources consumed by a
single quantification process.  One approach to solving this problem is
``divide and conquer''.  By breaking the matrices up row- or column-wise,
larger batches of microarrays can be co-processed \cite{aroma}.  While this is
useful for performing analysis on a static data set, it does not address the
needs of users performing analysis on constantly growing data sets, because
each time an additional microarray is added the entire set must be reprocessed.
Katz \emph{et al.} \cite{genelogic} recognized that the true advantage of
multi-array over single-array quantification methods is that they fit probe and
probeset behavior to a model, and that if a theoretical (as opposed to
emperical) model is used and reasonable estimates can be calculated and saved,
arrays can be added one by one without reprocessing the whole set.  A similar
system was implemented in Celsius, and is described in greater detail in
Chatper \celsiuschapter \cite{celsius}.
