%%% XXX old outlining, can probably be tossed.
%\section{Microarray Informatics}
%\subsection{Data Storage}
%\subsection{Data Retrieval}
%\subsection{Interoperability}
%\subsubsection{Data Encoding}
%\subsubsection{Data Modeling}
%\subsubsection{Exchange Protocols}
%\subsection{Scalability Concerns In Microarray Informatics}
%%% XXX end old outlining

%%% XXX FIXME what about these????
%\section{Infrastructure}
%\subsection{Resource Administration}
%\subsection{Package Management}
%\subsection{Configuration Management}
%%% XXX

citations to merge in:
\subsubsection{Maintenance}

The cost of high-performance, horizontal systems is increased complexity.

rocks           \cite{rocks}
puppet          \cite{puppet}
rpm             \cite{rpm}

\subsection{Storage}

%See Also
postgres        \cite{postgresql}
chado           \cite{chado}

\subsubsection{Data Representation}
%\subsubsubsection{Assay Measurements}
%\subsubsubsection{Annotations}

%See Also
mged ontology           \cite{mo}
mp ontology             \cite{mp}
sequence ontology       \cite{so}
cell ontology           \cite{cl}
ncbo                    \cite{ncbo}
chado                   \cite{chado}
mage-ml                 \cite{mage}

\subsubsection{Performance}
%\subsubsubsection{Hardware}
%\subsubsubsection{Clustering}
%\subsubsubsection{Partitioning}
%\subsubsection{Computation}
%\subsubsubsection{Data Loading}
etl                     \cite{kimball2004}
%\subsubsubsection{Data Analysis}
\subsection{Data Sharing}

%See Also
das                     \cite{das}
nation                  \cite{nation}
spice                   \cite{spice}
mage-ml                 \cite{mage}

\subsubsection{Data Representation}
%\subsubsubsection{Standards}
%\subsubsubsection{XML}
\subsubsection{MAGE}
\label{MAGE}
MAGE-OM and MAGE-ML

das                     \cite{das}
ncbo                    \cite{ncbo}
mage-ml                 \cite{mage}
%\subsubsubsection{URL}
%\subsubsubsection{Ontology}
mged ontology           \cite{mo}
mp ontology             \cite{mp}
sequence ontology       \cite{so}
cell ontology           \cite{cl}
ncbo                    \cite{ncbo}
%\subsubsubsection{Syntax}
%\subsubsubsection{Data}
%\subsubsubsection{Normalization}
%\subsubsubsection{Reproducibility}
%\subsubsubsection{Modularity}



\subsection{Data Modeling}
\label{Data Modeling}

In order to be able to perform analyses on the results of a DNA microarray
experiment, the digitized assay data (Section \ref{Image Processing}) must be
structured and stored.  This is also true of the experimental \emph{metadata},
or description of the experimental design and procedures.  Principles of
scalability and mass-production prescribe that the encoding of these data be
uniform, meaning that the structure used to encode the experiment must be
sufficiently flexible and descriptive to capture the full range of experiments
that can be conceived.

The \emph{MicroArray Gene Expression} (\emph{MAGE}) Group was established by
the \emph{MicroArray Gene Expression Data Society} (\emph{MGEDS}, sometimes
also referred to as \emph{MGED}) to develop a standard for the representation
of microarray data, with the intent of making those data exchangeable between
different information systems \cite{mged}.  The work of the MAGE Group can be
divided into two types of projects: those dealing with the syntax used to
concretely represent microarray data, and those dealing with the semantics used
to describe the microarray experimental materials and processes, produced data,
and data derived from processing \cite{miame, mage, mo}.

The semantic developments of the MAGE Group \cite{mage} are more important for
\dbthesis than the syntactic developments.  This is because semantics are
abstract and generally useful for encoding experimental information, while the
syntactic developments of the MAGE Group have optimized for the purposes of
data interchange using technologies such as the \emph{eXtensible Markup
Language} (\emph{XML}) that are not well-suited for systems whose primary
purpose is to store and retrieve large volumes of quantitative data.

Application of the MGED semantic technologies, such as the MGED Ontology
\cite{mo}, are discussed elsewhere in this document (Section \ref{Label
Encoding}).  There are two major semantic developments from MGEDS.  The first
of these is the \emph{MAGE Object Model}, or \emph{MAGE-OM}.  The purpose of
the MAGE-OM is to provide a standard set of classes that can be used to
represent any object or process that may be included in, or referenced from, a
microarray experiment \cite{mage}.  It draws heavily from a seminal work by
Brazma \emph{et al.} describing the ``Minimal Information About a Microarray
Experiement'', or \emph{MIAME} that should be collected \cite{miame}.  The
model employs concepts common to object-oriented software design and knowledge
engineering, such as subclass/superclass relationships between object classes,
the notion of abstract classes, and the possibility for directional and
cardinal relationships between objects.  The specification of the MAGE-OM was
developed using the \emph{Unified Modeling Language} (\emph{UML}), a standard
technology used by knowledge and software engineers for the composition of
object models.  Fitting microarray experiment data into the MAGE Object Model
is described in greater detail in Section \ref{Experimental Metadata}.
Encoding specific information about objects under investigation or that are
used to facilitate the conduct of an experiment are discussed in Section
\ref{Object Metadata}.

Unique syntax-related challenges that have arisen as part of \dbthesis are also
presented.  Internal data representation and scalability issues are discussed
in Section \ref{Storage} and interoperability concerns are discussed in greater
detail in Section \ref{Protocol}.

\subsubsection{Experimental Metadata}
\label{Experimental Metadata}

As a concrete example of how an experiment might be encoded into a MAGE-ML
document, consider the now-classical study of leukemias by Golub, et al
\cite{golub}.  Briefly, this study is has both pattern-detection and
predictive-modeling methodology (Sections \ref{Pattern
Detection}-\ref{Predictive Modeling}), and describes a method for identifying
features that discriminate between two leukemia subclasses and how they can be
used to identify the subclass of previously unlabeled cancer samples.

Encoded into the MAGE-OM, the initial cancer samples in Golub, et al
\cite{golub} are represented as \emph{BioSource} objects, a class used for
biological material prior to any treatment.  Each BioSource object then goas
through a series of modifications, ultimately resulting in a
\emph{LabeledExtract} object that represents the fluorescently-labeled cRNA
that is hybridized onto a microarray.  In the series of modifications, a
combination of a \emph{BioSample} object and one or more \emph{Treatment}
objects is used to represent the transformed Biosource object.  Further, each
LabeledExtract refers back to the object from which it was derived all the way
back to the BioSource object so that the full path of derivation via treatment
is modeled.

To represent the microarray hybridization, a \emph{BioAssay} object is created.
The BioAssay is a central connection point in the object model.  It refers to
an \emph{Array} object representing the microarray itself, the LabeledExtract
that is hybridized, and to one or more \emph{Factor} objects.  The factor
objects are in turn related to a network of other objects that encode the
design and variables used in the microarray experiment.  The Array object is
associated to a series of other objects that describe the microarray itself,
including the information about specific sequences and their physical locations
on the microarray, as well as information about the grouping of features on the
array used as reporters for a common cRNA target sequence.  Further, each
LabeledExtract is associated with a \emph{Channel} object.  The Channel is used
to link a specific LabeledExtract to a specific \emph{Image} object that
results from the scanning of the hybridized array.

The Image object is combined with a \emph{FeatureExtraction} object to produce
a \emph{BioAssayData} object.  This association of objects represents that
transformation of the microarray image acquired by the scanner (Section
\ref{Assay}) into a numerical form that can be processed (Section \ref{Data
Processing}) for further analysis.  BioAssayData objects may also be derived
from other BioAssayData objects, similar to the way BioSource, BioSample, and
LabeledExtract objects may be related.  This is how the MAGE-OM represents
arbitrary data transformations, and is sufficient for describing microarray
data pre-processing (Section \ref{Data Processing}), as well as additional
downstream summarizations or other transformations of these data, such as
sample or probe set clustering.

\subsubsection{Object Metadata}
\label{Object Metadata}

Objects in the MAGE-OM may have attributes attached to them to provide
more specific detail about the microarray experiment.  A design decision was
made to reference, from the MAGE-OM, objects from an \emph{ontology} for the
description of objects.  Doing so constrains the scope of MAGE-OM's purpose to
the structure of the microarray experiment and associated quantitative data.

%CITE http://ksl-web.stanford.edu/knowledge-sharing/papers/onto-design.rtf
Ontologies are a key crossover into the biological sciences from computer
science, information theory, and artificial intelligence research.  According
to T. Gruber \cite{XXX}, an ontology is ``an explicit specification of some
topic. For our purposes, it is a formal and declarative representation which
includes the vocabulary (or names) for referring to the terms in that subject
area and the logical statements that describe what the terms are, how they are
related to each other, and how they can or cannot be related to each other.
Ontologies therefore provide a vocabulary for representing and communicating
knowledge about some topic and a set of relationships that hold among the terms
in that vocabulary.''

%CITE http://bioinformatics.oxfordjournals.org/cgi/content/full/22/7/866
As biologists have always faced the problems of nomenclature and
classification, ontologies are a natural extension of these activities.  Many
high-profile ontologies have been created to annotate and classify a wide
variety of biological concepts.  The \emph{MGED Ontology} (\emph{MO}) was
developed by MGED specifically for describing the attributes of objects used or
studied as part of an experiment \cite{XXX}, and as a formal encoding of the
concepts of the ``Minimal Information About a Microarray Experiment''
(\emph{MIAME}) developed by Brazma, et al \cite{miame}.  The MO is used
in \dbthesis for high-level grouping of experiments by their general design
(time-course, dose/response, etc).

Other ontologies are also useful in \dbthesis for attaching concepts to objects
that are outside the scope of the MO.  For example, we have used the Sequence
Ontology (SO) \cite{so} to encode information pertinent to the sequences
present on, or used to select the sequences present on, microarrays.  In
the case of using the SO, the concepts from the ontology are non-orthogonal to
those in the MO and therefore there is some redundancy in the annotation
stored.  In most cases, however, the ontologies used to annotate objects in the
data warehouse are orthogonal.  We have used the Adult Mouse Anatomy Ontology
(MA) \cite{ma} for attaching tissue information onto mammalian samples, the
Mammalian Pathology Ontology (MPATH) \cite{mpath} for annotating disease
states, and the Cell Ontology (CL) \cite{cl} for annotating cell type onto
samples that have been grown \emph{in-vitro}.

The use of ontologies provides a mechanism for attaching consistent and
unambiguous descriptions to objects relevant to \dbthesis.  The encodings are
useful in that they allow for the accurate representation, storage (Section
\ref{Storage}), retrieval, and exchange (Section \ref{Protocol}) of information
about these objects.  Encoding object descriptions as ontologiy terms also
presents interesting data analysis opportunities because of the ontology
structure.   The relationship between terms in the ontologies is structured as
a directed graph, a data structure whose properties are well understood and for
which a large body of theory and analytical methods already exist.  Indeed,
several prior studies have leveraged the annotation of genes into the Gene
Ontology (GO) \cite{go} to predict gene function \cite{shortestpath, pachinko,
termtissue}, and these techiques can be extrapolated to annotation of the
hybridized biological source materials as well.

Ontology construction continues to be an active area of development in the
biological sciences, and many important projects have spearheaded the effort to
establish ontologies for the biological research community.  The \emph{Open
Biomedical Ontologies} (\emph{OBO}) project provides a range of ontologies
designed for use in the biomedical fields \cite{obo}.  Other related projects
include the Gene Ontology, and the Sequence Ontology \cite{go,so}.  Recently,
the National Institutes of Health recognized the critical role ontologies are
playing in the organization andinterchange of biological information and
founded the National Center for Biomedical Ontology \cite{ncbo} as a
coordinating center for ontologies, similar to what the \emph{National Center
for Biological Information} (\emph{NCBI}) \cite{ncbi} and \emph{PubMed}
\cite{pubmed} do for sequence and bibliographical data.

\subsection{Data Storage \& Retrieval}
\label{Storage}

The data and metadata produced as part of pre-processing (Section \ref{Data
Processing}) and fitting the microarray experiment to a uniform data model
(Section \ref{Data Modeling}) must be stored and made available for retrieval
at a later time to analysts to perform further processing.

Storage solutions borrowed from computer science and the information technology
industry include physical media on which to store the data, such as hard
drives, and also database systems to structure the data in accessible and
searchable contexts.  Hard drive storage systems have advanced considerably
over the decades in response to the demand for safe, reliable, and cost
effective ways of storing large amounts of data.  Systems that link together
many individual hard drives or storage on groups of computers into contiguous
virtual volumes are available, making it possible to group together large
datasets in a common repository.  Implementations of contiguous volumes, such
as redundant arrays of inexpensive disks (RAID) and storage area networks
(SANs) make the storage of critical scientific data secure and retrieval
speedy.  Database systems represent another strategy for meeting the storage
needs of bioinformatics projects.  Unlike hard disk-based solutions, databases
actively index information to improve the retrieval of structured data.
Advances in open source projects such as MySQL (\url{http://www.mysql.com}) and
PostgreSQL (\url{http://www.postgresql.org}), have allowed researchers to use
very high performance relational database systems in their research for minimal
cost.

Many database solutions exist for representing biological data.  The Generic
Model Organism Database Project (GMOD, \url{http://www.gmod.org}) provides the
modular Chado schema \cite{chado} for storing a wide variety of biological
data.  This schema has been used by a variety of projects, and is used as the
primary relational database schema in \dbthesis.  Details of how the Chado
database schema is used in \dbthesis is given in Chapter \dbthesis.

\subsection{Interoperability \& Exchange Protocols}
\label{Protocol}

\subsection{Computing Infrastructure}
\label{Computing Infrastructure}









%%% XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\subsubsection{Annotations}

%@ annotation -> ontologies
Ontologies represent another key crossover from computer science, information
theory, and artificial intelligence research.  An ontology is a system of
related terms that is intended to encapsulate the classification of a system.
They are a way of describing and understanding a facet of the world and, as such,
serve as natural points of standardization.  As biologists have always faced
the problems of nomenclature and classification, ontologies are a natural
extension of these activities.  Many high-profile ontologies have been created
to annotate and classify a wide variety of biological concepts.  For
microarrays, much work has gone into defining standards (such as MAGE-ML and
MIAME) that structure the task of recording information about experimental
design \cite{miame,mage}. Extensive work has also resulted in the use of
ontologies to describe biological systems and the experiments performed on
them.  The MAGE ontology is of particular importance in the microarray field
but others exist such as the extremely useful gene ontology (GO) that aims to
structure the annotations on gene function and nomenclature \cite{mo,go}.

Many important projects have spearheaded the effort to establish ontologies for
the biological research community.  The open biomedical ontologies (OBO)
project provides a range of ontologies designed for use in the biomedical
fields.  Other related projects include GO, the MGED ontology, and the sequence
ontology (SO) \cite{mo,go,so}.  Several organization exist to support the
development of new and existing ontologies including the National Center for
Biomedical Ontology \url{http://www.bioontology.org}, MGED
\url{http://www.mged.org}, and OBO \url{http://obo.sourceforge.net}
\cite{ncbo,obo}, as well as many organism-specific databases
\cite{flybase,wormbase,sgd}.  Ontologies from these projects have been made
available in Chado and are used extensively in the Celsius project to annotate
microarray experiments with disease type, tissue type, and other important
information.  The Celsius system is described in more detail in Chatper
\celsiuschapter.


\subsubsection{Analysis}

%@ analysis -> common APIs
In microarrays, many different algorithms exist that allow a researcher to
process the raw information from a hybridization into meaningful gene
expression data.  This process, called normalization and standardization, can
range from a simple scaling to sophisticated algorithms that model probe
hybridization efficiency and integrate data from multiple experiments to arrive
at a more accurate value. With many different options, the need for
standardizing a mechanism to interact with these algorithms becomes apparent.
Without common application programming interfaces (APIs), each distinct
algorithm would require its own input and output format and require many more
hours to be spent on the trivial task of data formating.  Instead, good
programming practices from computer science dictate coding abstraction and
encapsulation.  These ideas can be seen in the Bioconductor or Bioperl APIs for
biological data and algorithms \cite{bioperl,bioc}.  For
microarrays this translates to a common framework for accessing many different
normalization techniques with the same programmatic calls, only the flag
specifying what algorithm to be used changes.  Similarly, the Seq::IO section
of the Bioperl API allows sequence data to be read and written to and from a
variety of sequence file formats \cite{bioperl}.  This allows programs
to access many different formats of data without needing to be rewritten.
Common APIs clearly are favored by both the open source community and the
bioinformatics community for these reasons.

\subsubsection{Sharing}\label{Sharing}

%@ sharing -> web services
Related to the concept of open APIs for interacting with biological data is the
concept of web services.  Sharing of biological data, whether it is raw,
primary data, or processed results, is of the utmost concern for biologist.
Yet journal articles provide a poor repository for large datasets.  Simply
downloading large data files is a better approach, yet many times only a few
pieces of information are needed and the downloading of an entire dataset is
unnecessary.  For example, if a researcher wants a particular protein structure
from the protein data bank (PDB, \url{http://www.pdb.org}) downloading the
thousands of structures in bulk from the PDB is unnecessary.

The emerging web services approach used across the Internet points to a better
solution.  This model includes technology such as the simple object access
protocol and REST concepts for interacting with an API remotely over the
hypertext transport protocol (HTTP) on which the web is based.  This idea is
that simple requests for data or information calculated on the fly can be made
by a researcher and the result is calculated or retrieved remotely and returned
over the Internet.  This type of approach is extremely flexible and can be used
in a variety of contexts to present biological data to other researchers.

The distributed annotation system, or DAS, is a popular bioinformatics web
services project geared towards the sharing of genome annotations with the
larger research community \cite{das}.  Organizations looking to share
genome assemblies, gene annotations, and other genomic features use DAS to make
this information available over the web.  Implementations of DAS use the
standard HTTP protocol and XML as an exchange standard.  The next version,
DAS/2, expands on the genomics focus of DAS by including capabilities to
exchange ontologies, download experimental assay results such as microarray
data, and perform on-demand sequence analysis such as BLAST.  The success of
DAS as a project is due to the ease of which scientists can utilize information
published with DAS.  Many clients exist, such as GBrowse and IGB, and the web
services model affords programmatic access to the servers \cite{gbrowse}.
This allows additional applications to be built on top of these public
repositories.  For example, the Celsius project web interfaces were created
on top of a DAS/2 server which provided the raw data.  These web tools let an
end user query the microarray data available via ontology annotations and
download the corresponding data in a variety of different processed forms.

More information on web services can be found in Chapters \XXXchapter,
\gmodwebchapter, and the Appendix. Chapter \XXXchapter\ details
the creation of a web services designed to thread protein sequences onto know
structures with the end goal of identifying disulfide bonds.  Chapter
\gmodwebchapter\ examines a model organism website generation framework that
includes web services tools.

\subsubsection{Standardization}

%@ standardization -> software packaging
A closely related concept to web services is the concept of software
standardization.  In the model of web services, a researcher can focus on the
analysis of data and its biological meaning rather than figuring out how to
store data locally.  This approach affords abstraction between the researcher
and the entity providing the web service, making it easier for others to
either validate existing work by performing the same analysis or expand on the
work using the same web services.  It allows researchers to easily standardize
a given dataset or analysis server.  Another technique familiar to all
computer users on standardization is the versioning of computer programs.  When
research is being performed on a particular dataset or with a particular
software program, it is extremely important to track which version was
used.  Otherwise it becomes impossible to replicate the work.  The idea of
software packaging, borrowed from the field of information technology, is of
key importance to bioinformatics.  In addition to simply versioning software,
many comprehensive systems exist for specifically tracking, installing, and
updating both software and data in a particular computing environment.  The
Linux system, for example, uses one of several different package managers to
perform this task.

The Biopackages project looks to standardize many tools used commonly in
bioinformatics projects. It encompasses an automated build system that creates
software packages for particular Linux distributions.  These include packages
for APIs such as BioPerl and BioConductor, web services such as DAS/2, and
databases such as Chado.  Details of the construction, public availability, and
benefits of this standardization tool can be found in Chapter \biopackageschapter.

%@ I have worked on several software solutions for these (maybe just integrate
%these above) This can just be a "Chapter X shows..." paragraph that maps the
%topics above to chapters. I'm just going to include these above.

\subsection{Further Work}

%@ Future biologies will be driven by basic needs as biology continues to
%become integrated with other disciplines.
Biology will continue to be driven by the basic needs to store, annotate,
analyze, share, and standardize biological data and practices. As biology
continues to become more integrated with other disciplines and influences the
development of other fields at the same time, new and effective technologies
will continue to be developed. In this section, many facets of computer science
and information technology were explored as they relate to the common problems
facing biologists today and in the future.  The use of computer science and
information technology continues to expand in biological research and provides
new avenues to address challenges and demands of this transforming field.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MORE FRAGMENTS
%\section{Infrastructure}
\label{Infrastructure}

The study of biology is rapidly evolving.  Bottom-up approaches, in which data
are produced individually and slowly are being replaced by high-throughput
methods that produce, in parallel, large volumes of high-dimensional,
high-resolution measurements.  Recent and emerging technologies such as DNA
microarrays, mass spectrometry, and next-generation DNA sequencing enable the
biologist to study the molecular details of entire organisms rather than just a
few genes.  This transition has created new opportunities for discovery in
collaboration with engineers, mathematicians, statisticians, and computer
scientists.  At the same time, this transition necessitates an overhaul of the
computational infrastructure used by biologists.  Specifically, new approaches and
models are required for representing, storing, processing, and distributing
data produced by high-throughput methods.

%\subsection{Scalability \& Performance}
\label{Scalability & Performance}

One of the key principles that has enabled the creation of high-throughput
methods is scalability.  Simply put, scalable methods are able to process large
amounts of data at the same level of performance as small amounts.  For
biological assays this is largely a problem of parallelization of chemical
reactions and miniaturization of devices.  In terms of computational
infrastructure, building effective, scalable systems also requires
parallelization.  This is often referred to by scalability engineers as
\emph{horizontal scaling} \cite{schlossnagle2006,arlitt2001}.

When setting up an infrastructure system using the \emph{horizontal scaling}
pattern, computer resources can be treated as groups of resources, or
\emph{clusters}.  Each cluster is responsible for one or more types of tasks,
and each component of the cluster is uniform.  This allows the throughput of
the cluster to be scaled simply by adding or removing components.

%See Also
genelogic		\cite{genelogic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Informatics}
%@ The role of informatics in biology is dramatically changing the discipline
%and opening up the applications of high-throughput, multiplexing technology
The field of biology is in the process of rapidly changing from a discipline
that largely focuses on bottom up approaches to one that produces and analyzes
vast amounts of high-throughput, high-dimensional data. New emerging
technologies such as DNA microarrays, mass spectrometry, and rapid genome
sequencing mean that a biologist can now study entire organisms rather than just
one or two genes at a time.  This transition has created exciting opportunities
for integrating a wide variety of previously separate research areas including
engineering, statistics, computer science, and other disparate subjects.  The
end result is a field that is swiftly adopting new technologies and approaches
to analyze these highly integrated and challenging datasets. 

%@ Biologists today are facing a huge problem of data sharing, organizing,
%analysing, etc.  Needs solutions to a host of problems.
% Biologists today face huge problems of storing data and organizing large
% datasets such as microarrays.  The typical microarray hybridization experiment
% on the Affymetrix U133A platform contains approximately 22,000 features and
% individual raw image files can take up to 150Mb each of storage space each.
% Pre-processed data files that summarize image files take up less space but
% large datasets can still become difficult to work with.  The Celsius project,
% for example, mirrors public microarray respositories allowing for research into
% comparisons between microarrays from thousands of experiments.  The processed
% expression file for this conglomerate dataset is over 5Tb which is
% prohibatively large for most research labs to duplicate. Furthermore, the
% actual analysis of very large datasets from mass spec or microarray
% technologies can tax computer RAM and CPU capabilities.  The all-by-all
% comparison of probeset correlation, for example, is currently intractable on
% the Celsius dataset on current hardware without first dividing the task into
% smaller steps. For problems where a natural parellel solution is not possible
% limitations of CPU and RAM during analysis become acute issues.

%\subsection{Informatic Challenges in Biology}

%@ These core problems break down into several related but distinct activites
% The informatics problems facing biologists can be divided into several distinct
% classes. Issues of working with microarray data can be thought of
% as a microacosum of the common types of problems biologist are experiencing as
% a result of new technologies.  A
% scientist studying gene expression patterns with a large number of microarrays
% experiments must be able to store both the raw and processed results.  These
% might include image files, processed expression data, normalized gene
% expression values, and other derived data.  Experimental annotations must be
% collected that clearly identify samples, how they were treated, and the
% parameters of the physical assay. Next, techniques for analyzing the data must
% be standardized.  This could include common algorithms or best practices for
% working with the data.  In microarrays, many competing analysis techniques are
% available and, regardless of the choice, a common way of accessing them is a key
% concern.  Furthermore, results can change with subsequent releases of new
% software or datasets. Since many scientific questions can take months or years
% to answer, keeping track of what resources were used to produce a given result
% are critical.  Microarray data can be normalized in a number of
% different ways with a variety of algorithms designed with similar but slightly
% different assumptions. Finally, sharing information is the final activity of
% most academic research.  Journals have become more sophisiticated in moving
% articles to completely digital distrubution and archiving models.  However as
% datasets continue to grow access via programatic interfaces is more and more
% important.  This allows for verification of existing results and also the
% synthesis of new questions and research directions.  Microarray data deposition
% in public repositories is a requirement for most journals yet automatic
% mechanisms for working with this data are lacking and requirements for what
% constitues primary data in this field are vague at best. The challenges facin

%@ Commonly applied solutions across the fields of information technology and
%computer science (collectively refered to as informatics) can meet many of
%these needs.  A hybrid approach is needed that blends the best resources from
%a variety of data driven desciplines with the highly parellel nature of modern
%genomics.
The issues facing biologists include storage, annotations, analysis, sharing, and
standardization.  All have roots in the highly technical and interdisciplinary
nature of modern biology.  Bioinformatics, computational biology, and other
related ``new'' fields take an interdisciplinary approach to solving these
challenges.  Commonly applied solutions across the fields of information
technology (IT), computer science, and engineering (collectively referred to
here as ``informatics'') can meet many of these needs.  A hybrid
approach that blends the best resources and practices from a variety
of data driven disciplines with the highly parallel nature of modern biology is needed.
Of particular note, the open source movement has achieved a high level of
success in creating free and flexible software that is philosophically aligned
with the principles of open, transparent, and freely accessible research.  It
is no wonder that most bioinformatics projects use open source software, such
as the Linux operating system, for the bulk of their work.  Solutions for many
of the technical challenges surrounding the transforming field of biology can
be found in the open source movement.

In the following sections, possible solutions to some of the specific informatics
challenges facing biology today are described. Summaries of the open source
bioinformatics projects in this dissertation are available in Table %XXX\ref{projects}.

%@ Informatic solutions borrowed from computer science include...
%@ These solutions can be adapted to the core problems...
