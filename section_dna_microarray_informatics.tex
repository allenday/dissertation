\subsection{Data Modeling}\label{Data Modeling}

In order to be able to perform analyses on the results of a DNA microarray
experiment, the digitized assay data (Section \ref{Image Processing}) must be
structured and stored.  This is also true of the experimental \emph{metadata},
or description of the experimental design and procedures.  Principles of
scalability and mass-production prescribe that the encoding of these data be
uniform, meaning that the structure used to encode the experiment must be
sufficiently flexible and descriptive to capture the full range of experiments
that can be conceived.

The \emph{MicroArray Gene Expression} (\emph{MAGE}) Group was established by
the \emph{MicroArray Gene Expression Data Society} (\emph{MGEDS}, sometimes
also referred to as \emph{MGED}) to develop a standard for the representation
of microarray data, with the intent of making those data exchangeable between
different information systems \cite{mged}.  The work of the MAGE Group can be
divided into two types of projects: those dealing with the syntax used to
concretely represent microarray data, and those dealing with the semantics used
to describe the microarray experimental materials and processes, produced data,
and data derived from processing \cite{miame, mage, mo}.

The semantic developments of the MAGE Group \cite{mage} are more important for
\dbthesis than the syntactic developments.  This is because semantics are
abstract and generally useful for encoding experimental information, while the
syntactic developments of the MAGE Group have optimized for the purposes of
data interchange using technologies such as the \emph{eXtensible Markup
Language} (\emph{XML}) that are not well-suited for systems whose primary
purpose is to store and retrieve large volumes of quantitative data.

Application of the MGED semantic technologies, such as the MGED Ontology
\cite{mo}, are discussed elsewhere in this document (Section \ref{Label
Encoding}).  There are two major semantic developments from MGEDS.  The first
of these is the \emph{MAGE Object Model}, or \emph{MAGE-OM}.  The purpose of
the MAGE-OM is to provide a standard set of classes that can be used to
represent any object or process that may be included in, or referenced from, a
microarray experiment \cite{mage}.  It draws heavily from a seminal work by
Brazma \emph{et al.} describing the ``Minimal Information About a Microarray
Experiement'', or \emph{MIAME} that should be collected \cite{miame}.  The
model employs concepts common to object-oriented software design and knowledge
engineering, such as subclass/superclass relationships between object classes,
the notion of abstract classes, and the possibility for directional and
cardinal relationships between objects.  The specification of the MAGE-OM was
developed using the \emph{Unified Modeling Language} (\emph{UML}), a standard
technology used by knowledge and software engineers for the composition of
object models.  Fitting microarray experiment data into the MAGE Object Model
is described in greater detail in Section \ref{Experimental Metadata}.
Encoding specific information about objects under investigation or that are
used to facilitate the conduct of an experiment are discussed in Section
\ref{Object Metadata}.

Unique syntax-related challenges that have arisen as part of \dbthesis are also
presented.  Internal data representation and scalability issues are discussed
in Section \ref{Storage} and interoperability concerns are discussed in greater
detail in Section \ref{Protocol}.

\subsubsection{Experimental Metadata}
\label{Experimental Metadata}

As a concrete example of how an experiment might be encoded into a MAGE-ML
document, consider the now-classical study of leukemias by Golub, et al
\cite{golub}.  Briefly, this study is has both pattern-detection and
predictive-modeling methodology (Sections \ref{Pattern
Detection}-\ref{Predictive Modeling}), and describes a method for identifying
features that discriminate between two leukemia subclasses and how they can be
used to identify the subclass of previously unlabeled cancer samples.

Encoded into the MAGE-OM, the initial cancer samples in Golub, et al
\cite{golub} are represented as \emph{BioSource} objects, a class used for
biological material prior to any treatment.  Each BioSource object then goas
through a series of modifications, ultimately resulting in a
\emph{LabeledExtract} object that represents the fluorescently-labeled cRNA
that is hybridized onto a microarray.  In the series of modifications, a
combination of a \emph{BioSample} object and one or more \emph{Treatment}
objects is used to represent the transformed Biosource object.  Further, each
LabeledExtract refers back to the object from which it was derived all the way
back to the BioSource object so that the full path of derivation via treatment
is modeled.

To represent the microarray hybridization, a \emph{BioAssay} object is created.
The BioAssay is a central connection point in the object model.  It refers to
an \emph{Array} object representing the microarray itself, the LabeledExtract
that is hybridized, and to one or more \emph{Factor} objects.  The factor
objects are in turn related to a network of other objects that encode the
design and variables used in the microarray experiment.  The Array object is
associated to a series of other objects that describe the microarray itself,
including the information about specific sequences and their physical locations
on the microarray, as well as information about the grouping of features on the
array used as reporters for a common cRNA target sequence.  Further, each
LabeledExtract is associated with a \emph{Channel} object.  The Channel is used
to link a specific LabeledExtract to a specific \emph{Image} object that
results from the scanning of the hybridized array.

The Image object is combined with a \emph{FeatureExtraction} object to produce
a \emph{BioAssayData} object.  This association of objects represents that
transformation of the microarray image acquired by the scanner (Section
\ref{Assay}) into a numerical form that can be processed (Section \ref{Data
Processing}) for further analysis.  BioAssayData objects may also be derived
from other BioAssayData objects, similar to the way BioSource, BioSample, and
LabeledExtract objects may be related.  This is how the MAGE-OM represents
arbitrary data transformations, and is sufficient for describing microarray
data pre-processing (Section \ref{Data Processing}), as well as additional
downstream summarizations or other transformations of these data, such as
sample or probe set clustering.

\subsubsection{Object Metadata}
\label{Object Metadata}

Objects in the MAGE-OM may have attributes attached to them to provide
more specific detail about the microarray experiment.  A design decision was
made to reference, from the MAGE-OM, objects from an \emph{ontology} for the
description of objects.  Doing so constrains the scope of MAGE-OM's purpose to
the structure of the microarray experiment and associated quantitative data.

%CITE http://ksl-web.stanford.edu/knowledge-sharing/papers/onto-design.rtf
Ontologies are a key crossover into the biological sciences from computer
science, information theory, and artificial intelligence research.  According
to T. Gruber \cite{XXX}, an ontology is ``an explicit specification of some
topic. For our purposes, it is a formal and declarative representation which
includes the vocabulary (or names) for referring to the terms in that subject
area and the logical statements that describe what the terms are, how they are
related to each other, and how they can or cannot be related to each other.
Ontologies therefore provide a vocabulary for representing and communicating
knowledge about some topic and a set of relationships that hold among the terms
in that vocabulary.''

%CITE http://bioinformatics.oxfordjournals.org/cgi/content/full/22/7/866
As biologists have always faced the problems of nomenclature and
classification, ontologies are a natural extension of these activities.  Many
high-profile ontologies have been created to annotate and classify a wide
variety of biological concepts.  The \emph{MGED Ontology} (\emph{MO}) was
developed by MGED specifically for describing the attributes of objects used or
studied as part of an experiment \cite{XXX}, and as a formal encoding of the
concepts of the ``Minimal Information About a Microarray Experiment''
(\emph{MIAME}) developed by Brazma, et al \cite{miame}.  The MO is used
in \dbthesis for high-level grouping of experiments by their general design
(time-course, dose/response, etc).

Other ontologies are also useful in \dbthesis for attaching concepts to objects
that are outside the scope of the MO.  For example, we have used the Sequence
Ontology (SO) \cite{so} to encode information pertinent to the sequences
present on, or used to select the sequences present on, microarrays.  In
the case of using the SO, the concepts from the ontology are non-orthogonal to
those in the MO and therefore there is some redundancy in the annotation
stored.  In most cases, however, the ontologies used to annotate objects in the
data warehouse are orthogonal.  We have used the Adult Mouse Anatomy Ontology
(MA) \cite{ma} for attaching tissue information onto mammalian samples, the
Mammalian Pathology Ontology (MPATH) \cite{mpath} for annotating disease
states, and the Cell Ontology (CL) \cite{cl} for annotating cell type onto
samples that have been grown \emph{in-vitro}.

The use of ontologies provides a mechanism for attaching consistent and
unambiguous descriptions to objects relevant to \dbthesis.  The encodings are
useful in that they allow for the accurate representation, storage (Section
\ref{Storage}), retrieval, and exchange (Section \ref{Protocol}) of information
about these objects.  Encoding object descriptions as ontologiy terms also
presents interesting data analysis opportunities because of the ontology
structure.   The relationship between terms in the ontologies is structured as
a directed graph, a data structure whose properties are well understood and for
which a large body of theory and analytical methods already exist.  Indeed,
several prior studies have leveraged the annotation of genes into the Gene
Ontology (GO) \cite{go} to predict gene function \cite{shortestpath, pachinko,
termtissue}, and these techiques can be extrapolated to annotation of the
hybridized biological source materials as well.

Ontology construction continues to be an active area of development in the
biological sciences, and many important projects have spearheaded the effort to
establish ontologies for the biological research community.  The \emph{Open
Biomedical Ontologies} (\emph{OBO}) project provides a range of ontologies
designed for use in the biomedical fields \cite{obo}.  Other related projects
include the Gene Ontology, and the Sequence Ontology \cite{go,so}.  Recently,
the National Institutes of Health recognized the critical role ontologies are
playing in the organization andinterchange of biological information and
founded the National Center for Biomedical Ontology \cite{ncbo} as a
coordinating center for ontologies, similar to what the \emph{National Center
for Biological Information} (\emph{NCBI}) \cite{ncbi} and \emph{PubMed}
\cite{pubmed} do for sequence and bibliographical data.

\subsection{Interoperability \& Exchange of Data}\label{Protocol}

Related to the concept of open APIs for interacting with biological data is the
concept of web services.  Sharing of biological data, whether it is raw,
primary data, or processed results, is of the utmost concern for biologist.
Yet journal articles provide a poor repository for large datasets.  Simply
downloading large data files is a better approach, yet many times only a few
pieces of information are needed and the downloading of an entire dataset is
unnecessary.  For example, if a researcher wants a particular protein structure
from the protein data bank (PDB) \cite{pdb} downloading the thousands of
structures in bulk from the PDB is unnecessary.

The emerging web services approach used across the Internet points to a better
solution.  This model includes technology such as SOAP, the Simple Object
Access Protocol and ReST concepts for interacting with an API remotely over the
hypertext transport protocol (HTTP) on which the web is based \cite{rest}.
This idea is that simple requests for data or information calculated on the fly
can be made by a researcher and the result is calculated or retrieved remotely
and returned over the Internet.  This type of approach is extremely flexible
and can be used in a variety of contexts to present biological data to other
researchers.

The distributed annotation system, or DAS, is a popular bioinformatics web
services project geared towards the sharing of genome annotations with the
larger research community \cite{das}.  Organizations looking to share genome
assemblies, gene annotations, and other genomic features use DAS to make this
information available over the web.  Implementations of DAS use the standard
HTTP protocol and XML as an exchange standard.  The next version, DAS/2,
expands on the genomics focus of DAS by including capabilities to exchange
ontologies, download experimental assay results such as microarray data, and
perform on-demand sequence analysis such as BLAST \cite{blast}.  The success of
DAS as a project is due to the ease of which scientists can utilize information
published with DAS.  Many clients exist, such as the Generic Genome Browser
because the web services model affords programmatic access to the servers
\cite{gbrowse, spice}.  This allows additional applications to be built on top
of these public repositories.  For example, the Celsius project web interfaces,
described in Chapter \celsiuschapter, were created on top of a DAS/2 server
which provided the raw data.  These web tools let an end user query the
microarray data available via ontology annotations and download the
corresponding data in a variety of different processed forms.

More information on web services can be found in Chapter \celsiuschapter and
Chapter \gmodwebchapter, which examines a model organism website generation
framework that includes web services tools.

\subsection{Computing Infrastructure}\label{Computing Infrastructure}

One of the key principles that has enabled the creation of high-throughput
methods is scalability.  Simply put, scalable methods are able to process large
amounts of data at the same level of performance as small amounts.  For
biological assays this is largely a problem of parallelization of chemical
reactions and miniaturization of devices.  In terms of computational
infrastructure, building effective, scalable systems also requires
parallelization.  This is often referred to by scalability engineers as
\emph{horizontal scaling} \cite{schlossnagle2006,arlitt2001}.

When setting up an infrastructure system using the \emph{horizontal scaling}
pattern, computer resources can be treated as groups of resources, or
\emph{clusters}.  Each cluster is responsible for one or more types of tasks,
and each component of the cluster is uniform.  This allows the throughput of
the cluster to be scaled simply by adding or removing components.

The effort needed to maintain each of a cluster of computers may exceed that
needed to maintain computers individually.  For example, additional software
may be necessary to orchestrate computation across the cluster \cite{rocks}.
Contention for storage and network bandwith may also be a concern.  Hardware
that is employed, while at face-value appears to be of identical make and
model, may in fact contain different versions of microchip sub-circuits,
resulting in inconsistent behavior.  Finally, there is the issue of managing
the software present on each computer within the cluster so that they produce
equivalent results.  As part of \dbthesis, we have had to deal directly with
the problems of storage and maintaining consistency of software on a computing
cluster.  Storage issues are discussed in Section \ref{Storage}, and the
problem of maintaining software consistency was addressed using Puppet
\cite{puppet} along with the construction of an RPM \cite{rpm} software
repository that is described in greater detail in Chatper \biopackageschapter.

\subsubsection{Standardization}

%@ standardization -> software packaging
A closely related concept to web services is the concept of software
standardization.  In the model of web services, a researcher can focus on the
analysis of data and its biological meaning rather than figuring out how to
store data locally.  This approach affords abstraction between the researcher
and the entity providing the web service, making it easier for others to either
validate existing work by performing the same analysis or expand on the work
using the same web services.  It allows researchers to easily standardize a
given dataset or analysis server.  Another technique familiar to all computer
users on standardization is the versioning of computer programs.  When research
is being performed on a particular dataset or with a particular software
program, it is extremely important to track which version was used.  Otherwise
it becomes impossible to replicate the work.  The idea of software packaging,
borrowed from the field of information technology, is of key importance to
bioinformatics.  In addition to simply versioning software, many comprehensive
systems exist for specifically tracking, installing, and updating both software
and data in a particular computing environment.  The Linux system, for example,
uses one of several different package managers to perform this task.

The Biopackages project looks to standardize many tools used commonly in
bioinformatics projects. It encompasses an automated build system that creates
software packages for particular Linux distributions.  These include packages
for APIs such as BioPerl \cite{bioperl} and BioConductor \cite{bioc}, web
services such as DAS/2 \cite{das}, and databases such as Chado \cite{chado}.
Details of the construction, public availability, and benefits of this
standardization tool can be found in Chapter \biopackageschapter.

\subsubsection{Storage}\label{Storage}

The data and metadata produced as part of pre-processing (Section \ref{Data
Processing}) and fitting the microarray experiment to a uniform data model
(Section \ref{Data Modeling}) must be stored and made available for retrieval
at a later time to analysts to perform further processing.

Storage solutions borrowed from computer science and the information technology
industry include physical media on which to store the data, such as hard
drives, and also database systems to structure the data in accessible and
searchable contexts.  Hard drive storage systems have advanced considerably
over the decades in response to the demand for safe, reliable, and cost
effective ways of storing large amounts of data.  Systems that link together
many individual hard drives or storage on groups of computers into contiguous
virtual volumes are available, making it possible to group together large
datasets in a common repository.  Implementations of contiguous volumes, such
as redundant arrays of inexpensive disks (RAID) and storage area networks
(SANs) make the storage of critical scientific data secure and retrieval
speedy.  Database systems represent another strategy for meeting the storage
needs of bioinformatics projects.  Unlike hard disk-based solutions, databases
actively index information to improve the retrieval of structured data.
Advances in open source projects such as MySQL (\url{http://www.mysql.com}) and
PostgreSQL (\url{http://www.postgresql.org}), have allowed researchers to use
very high performance relational database systems in their research for minimal
cost.

Many database solutions exist for representing biological data.  The Generic
Model Organism Database Project (GMOD, \url{http://www.gmod.org}) provides the
modular Chado schema \cite{chado} for storing a wide variety of biological
data.  This schema has been used by a variety of projects, and is used as the
primary relational database schema in \dbthesis.  Details of how the Chado
database schema is used in \dbthesis is given in Chapter \dbthesis.
