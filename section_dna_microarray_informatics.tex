%%% XXX old outlining, can probably be tossed.
%\section{Microarray Informatics}
%\subsection{Data Storage}
%\subsection{Data Retrieval}
%\subsection{Interoperability}
%\subsubsection{Data Encoding}
%\subsubsection{Data Modeling}
%\subsubsection{Exchange Protocols}
%\subsection{Scalability Concerns In Microarray Informatics}
%%% XXX end old outlining

%%% XXX FIXME what about these????
%\section{Infrastructure}
%\subsection{Resource Administration}
%\subsection{Package Management}
%\subsection{Configuration Management}
%%% XXX

citations to merge in:
\subsubsection{Maintenance}

The cost of high-performance, horizontal systems is increased complexity.

rocks           \cite{papadopoulos2003}
puppet          \cite{puppet}
rpm             \cite{bailey1997}

\subsection{Storage}

%See Also
postgres        \cite{postgresql}
chado           \cite{chado}

\subsubsection{Data Representation}
%\subsubsubsection{Assay Measurements}
%\subsubsubsection{Annotations}

%See Also
mged ontology           \cite{mo}
mp ontology             \cite{mp}
sequence ontology       \cite{so}
cell ontology           \cite{cl}
ncbo                    \cite{ncbo}
chado                   \cite{chado}
mage-ml                 \cite{mage}

\subsubsection{Performance}
%\subsubsubsection{Hardware}
%\subsubsubsection{Clustering}
%\subsubsubsection{Partitioning}
%\subsubsection{Computation}
%\subsubsubsection{Data Loading}
etl                     \cite{kimball2004}
%\subsubsubsection{Data Analysis}
\subsection{Data Sharing}

%See Also
das                     \cite{das}
nation                  \cite{nation}
spice                   \cite{spice}
mage-ml                 \cite{mage}

\subsubsection{Data Representation}
%\subsubsubsection{Standards}
%\subsubsubsection{XML}
\subsubsection{MAGE}
\label{MAGE}
MAGE-OM and MAGE-ML

das                     \cite{das}
ncbo                    \cite{ncbo}
mage-ml                 \cite{mage}
%\subsubsubsection{URL}
%\subsubsubsection{Ontology}
mged ontology           \cite{mo}
mp ontology             \cite{mp}
sequence ontology       \cite{so}
cell ontology           \cite{cl}
ncbo                    \cite{ncbo}
%\subsubsubsection{Syntax}
%\subsubsubsection{Data}
%\subsubsubsection{Normalization}
%\subsubsubsection{Reproducibility}
%\subsubsubsection{Modularity}



\subsubsection{Data Modeling}
\label{Data Modeling}

In order to be able to perform analyses on the results of a DNA microarray
experiment, the assay data collected must be structured and stored.  This is
also true of the experimental \emph{metadata}, or description of the
experimental design and procedures.  Principles of scalability and
mass-production prescribe that the encoding of these data be uniform, meaning
that the structure used to encode the experiment must be sufficiently flexible
and descriptive to capture the full range of experiments that can be conceived.

The \emph{MicroArray Gene Expression} (\emph{MAGE}) Group was established by
the \emph{MicroArray Gene Expression Data Society} (\emph{MGEDS}, sometimes
also referred to as \emph{MGED}) to develop a standard for the representation
of microarray data, with the intent of making those data exchangeable between
different information systems.  The work of the MAGE Group can be divided into
two types of projects: those dealing with the syntax used to concretely
represent microarray data, and those dealing with the semantics used to
describe the microarray experimental materials and processes, produced data,
and data derived from processing.

The semantic developments of the MAGE Group are more important for \dbthesis
than the syntactic developments.  This is because semantics are abstract and
generally useful for encoding experimental information, while the syntactic
developments of the MAGE Group have optimized for the purposes of data
interchange using technologies such as the \emph{eXtensible Markup Language}
(\emph{XML}) that are not well-suited for systems whose primary purpose is to
store and retrieve large volumes of quantitative data.

Application of the MGED semantic technologies are discussed in other sections
of this document.  There are two major semantic developments from MGEDS.  The
first of these is the \emph{MAGE Object Model}, or \emph{MAGE-OM}.  The purpose
of the MAGE-OM is to provide a standard set of classes that can be used to
represent any object or process that may be included in, or referenced from, a
microarray experiment.  The model employs concepts common to object-oriented
software design and knowledge engineering, such as subclass/superclass
relationships between object classes, the notion of abstract classes, and the
possibility for directional and cardinal relationships between objects.  The
specification of the MAGE-OM was developed using the \emph{Unified Modeling
Language} (\emph{UML}), a standard technology used by knowledge and software
engineers for the composition of object models.  Fitting microarray experiment
data into the MAGE Object Model is described in greater detail in Section
\ref{Experimental Metadata}.  Encoding specific information about objects under
investigation or that are used to facilitate the conduct of an experiment are
discussed in Section \ref{Object Metadata}.

Unique syntax-related challenges that have arisen as part of \dbthesis are
also presented.  Internal data representation and scalability issues are
discussed in Section \ref{Storage} and interoperability concerns are discussed in
greater detail in Section \ref{Protocol}.

\subsubsection{Experimental Metadata}
\label{Experimental Metadata}

As a concrete example of how an experiment might be encoded into a MAGE-ML
document, consider the now-classical study of leukemias by Golub, et al
\cite{golub}.  Briefly, this study is has both pattern-detection and
predictive-modeling methodology (Sections \ref{Pattern
Detection}-\ref{Predictive Modeling}), and describes a method for identifying
features that discriminate between two leukemia subclasses and how they can be
used to identify the subclass of previously unlabeled cancer samples.

Encoded into the MAGE-OM, the initial cancer samples in Golub, et al
\cite{golub} are represented as \emph{BioSource} objects, a class used for
biological material prior to any treatment.  Each BioSource object then goas
through a series of modifications, ultimately resulting in a
\emph{LabeledExtract} object that represents the fluorescently-labeled cRNA
that is hybridized onto a microarray.  In the series of modifications, a
combination of a \emph{BioSample} object and one or more \emph{Treatment}
objects is used to represent the transformed Biosource object.  Further, each
LabeledExtract refers back to the object from which it was derived all the way
back to the BioSource object so that the full path of derivation via treatment
is modeled.

To represent the microarray hybridization, a \emph{BioAssay} object is created.
The BioAssay is a central connection point in the object model.  It refers to
an \emph{Array} object representing the microarray itself, the LabeledExtract
that is hybridized, and to one or more \emph{Factor} objects.  The factor
objects are in turn related to a network of other objects that encode the
design and variables used in the microarray experiment.  The Array object is
associated to a series of other objects that describe the microarray itself,
including the information about specific sequences and their physical locations
on the microarray, as well as information about the grouping of features on the
array used as reporters for a common cRNA target sequence.  Further, each
LabeledExtract is associated with a \emph{Channel} object.  The Channel is used
to link a specific LabeledExtract to a specific \emph{Image} object that
results from the scanning of the hybridized array.

The Image object is combined with a \emph{FeatureExtraction} object to produce
a \emph{BioAssayData} object.  This association of objects represents that
transformation of the microarray image acquired by the scanner (Section
\ref{Assay}) into a numerical form that can be processed (Section \ref{Data
Processing}) for further analysis.  BioAssayData objects may also be derived
from other BioAssayData objects, similar to the way BioSource, BioSample, and
LabeledExtract objects may be related.  This is how the MAGE-OM represents
arbitrary data transformations, and is sufficient for describing microarray
data pre-processing (Section \ref{Data Processing}), as well as additional
downstream summarizations or other transformations of these data, such as
sample or probe set clustering.

\subsubsection{Object Metadata}
\label{Object Metadata}

Objects in the MAGE-OM may have attributes attached to them to provide
more specific detail about the microarray experiment.  A design decision was
made to reference, from the MAGE-OM, objects from an \emph{ontology} for the
description of objects.  Doing so constrains the scope of MAGE-OM's purpose to
the structure of the microarray experiment and associated quantitative data.

%CITE http://ksl-web.stanford.edu/knowledge-sharing/papers/onto-design.rtf
Ontologies are a key crossover into the biological sciences from computer
science, information theory, and artificial intelligence research.  According
to T. Gruber \cite{XXX}, an ontology is ``an explicit specification of some
topic. For our purposes, it is a formal and declarative representation which
includes the vocabulary (or names) for referring to the terms in that subject
area and the logical statements that describe what the terms are, how they are
related to each other, and how they can or cannot be related to each other.
Ontologies therefore provide a vocabulary for representing and communicating
knowledge about some topic and a set of relationships that hold among the terms
in that vocabulary.''

%CITE http://bioinformatics.oxfordjournals.org/cgi/content/full/22/7/866
As biologists have always faced the problems of nomenclature and
classification, ontologies are a natural extension of these activities.  Many
high-profile ontologies have been created to annotate and classify a wide
variety of biological concepts.  The \emph{MGED Ontology} (\emph{MO}) was
developed by MGED specifically for describing the attributes of objects used or
studied as part of an experiment \cite{XXX}, and as a formal encoding of the
concepts of the ``Minimal Information About a Microarray Experiment''
(\emph{MIAME}) developed by Brazma, et al \cite{brazma2001mim}.  The MO is used
in \dbthesis for high-level grouping of experiments by their general design
(time-course, dose/response, etc).

Other ontologies are also useful in \dbthesis for attaching concepts to objects
that are outside the scope of the MO.  For example, we have used the Sequence
Ontology (SO) \cite{so} to encode information pertinent to the sequences
present on, or used to select the sequences present on, microarrays.  In
the case of using the SO, the concepts from the ontology are non-orthogonal to
those in the MO and therefore there is some redundancy in the annotation
stored.  In most cases, however, the ontologies used to annotate objects in the
data warehouse are orthogonal.  We have used the Adult Mouse Anatomy Ontology
(MA) \cite{ma} for attaching tissue information onto mammalian samples, the
Mammalian Pathology Ontology (MPATH) \cite{mpath} for annotating disease
states, and the Cell Ontology (CL) \cite{cl} for annotating cell type onto
samples that have been grown \emph{in-vitro}.

The use of ontologies provides a mechanism for attaching consistent and
unambiguous descriptions to objects relevant to \dbthesis.  The encodings are
useful in that they allow for the accurate representation, storage (Section
\ref{Storage}), retrieval, and exchange (Section \ref{Protocol}) of information
about these objects.

Encoding object descriptions as ontologiy terms also presents interesting
data analysis opportunities because of the ontology structure.   The
relationship between terms in the ontologies is structured as a directed graph,
a data structure whose properties are well understood and for which a large
body of theory and analytical methods already exist.  Indeed, several prior
studies have leveraged the annotation of genes into the Gene Ontology (GO)
\ref{go} to XXX and XXX \cite{XXX,XXX}.

Ontology development continues to be an active area of development in the
biological sciences, and many important projects have spearheaded the effort to
establish ontologies for the biological research community.  The \emph{Open
Biomedical Ontologies} (\emph{OBO}) project provides a range of ontologies
designed for use in the biomedical fields.  Other related projects include the
Gene Ontology, and the Sequence Ontology \cite{go,so}.  Recently, the National
Institutes of Health recognized the critical role ontologies are playing in the
organization andinterchange of biological information and founded the National
Center for Biomedical Ontology \cite{ncbo} as a coordinating center for
ontologies, similar to what the \emph{National Center for Biological
Information} (\emph{NCBI}) \cite{ncbi} and \emph{PubMed} \cite{pubmed} do for
sequence and bibliographical data.

\subsection{Data Storage \& Retrieval}
\label{Storage}

The data and metadata produced as part of pre-processing (Section \ref{Data
Processing}) and fitting the microarray experiment to a uniform data model
(Section \ref{Data Modeling}) must be stored and made available for retrieval
at a later time to analysts to perform further processing.

Storage solutions borrowed from computer science and information technology
include physical media on which to store the data, such as hard drives, and
also a database system to structure the data in accessible and searchable
contexts.  Hard drive storage systems have advanced considerably over the
decades in response to the demand for safe, reliable, and cost effective ways
of storing large amounts of data.  Systems that link together many individual
hard drives or storage on groups of computers into contiguous virtual volumes
are available, making it possible to group together large datasets in a common
repository.  Implementations of contiguous volumes, such as redundant arrays of
inexpensive disks (RAID) and storage area networks (SANs) make the storage of
critical scientific data secure and retrieval speedy.  Database systems
represent another strategy for meeting the storage needs of bioinformatics
projects.  Unlike hard disk-based solutions, databases actively index
information to improve the retrieval of structured data.  Advances in open
source projects such as MySQL (\url{http://www.mysql.com}) and PostgreSQL
(\url{http://www.postgresql.org}), have allowed researchers to use very high
performance relational database systems in their research for minimal cost.

Many database solutions exist for representing biological data.  The Generic
Model Organism Database Project (GMOD, \url{http://www.gmod.org}) provides the
modular Chado schema \cite{chado} for storing a wide variety of biological
data.  This schema has been used by a variety of projects, and is used as the
primary relational database schema in \dbthesis.  Details of how the Chado
database schema is used in \dbthesis is given in Chapter \dbthesis.

\subsection{Interoperability \& Exchange Protocols}
\label{Protocol}

\subsection{Computing Infrastructure}
\label{Infrastructure}









%%% XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\subsubsection{Annotations}

%@ annotation -> ontologies
Ontologies represent another key crossover from computer science, information
theory, and artificial intelligence research.  An ontology is a system of
related terms that is intended to encapsulate the classification of a system.
They are a way of describing and understanding a facet of the world and, as such,
serve as natural points of standardization.  As biologists have always faced
the problems of nomenclature and classification, ontologies are a natural
extension of these activities.  Many high-profile ontologies have been created
to annotate and classify a wide variety of biological concepts.  For
microarrays, much work has gone into defining standards (such as MAGE-ML and
MIAME) that structure the task of recording information about experimental
design \cite{brazma2001mim}. Extensive work has also resulted in the use of
ontologies to describe biological systems and the experiments performed on
them.  The MAGE ontology is of particular importance in the microarray field
but others exist such as the extremely useful gene ontology (GO) that aims to
structure the annotations on gene function and nomenclature
\cite{stoeckert2003mof,ashburner2000got}.

Many important projects have spearheaded the effort to establish ontologies for
the biological research community.  The open biomedical ontologies (OBO)
project provides a range of ontologies designed for use in the biomedical
fields.  Other related projects include GO, the MGED ontology, and the sequence
ontology (SO) \cite{stoeckert2003mof,ashburner2000got,eilbeck2006sot}.  Many
organization exist to support the development of new and existing ontologies
including the National Center for Biomedical Ontology
\url{http://www.bioontology.org}, MGED \url{http://www.mged.org}, and OBO
\url{http://obo.sourceforge.net}.  Ontologies from these projects have been
made available in Chado and are used extensively in the Celsius project to
annotate microarray experiments with disease type, tissue type, and other
important information.  The Celsius system is described in more detail in
the Appendix.


\subsubsection{Analysis}

%@ analysis -> common APIs
In microarrays, many different algorithms exist that allow a researcher to
process the raw information from a hybridization into meaningful gene
expression data.  This process, called normalization and standardization, can
range from a simple scaling to sophisticated algorithms that model probe
hybridization efficiency and integrate data from multiple experiments to arrive
at a more accurate value. With many different options, the need for
standardizing a mechanism to interact with these algorithms becomes apparent.
Without common application programming interfaces (APIs), each distinct
algorithm would require its own input and output format and require many more
hours to be spent on the trivial task of data formating.  Instead, good
programming practices from computer science dictate coding abstraction and
encapsulation.  These ideas can be seen in the Bioconductor or Bioperl APIs for
biological data and algorithms \cite{stajich2002btp,gentleman2006bos}.  For
microarrays this translates to a common framework for accessing many different
normalization techniques with the same programmatic calls, only the flag
specifying what algorithm to be used changes.  Similarly, the Seq::IO section
of the Bioperl API allows sequence data to be read and written to and from a
variety of sequence file formats \cite{stajich2002btp}.  This allows programs
to access many different formats of data without needing to be rewritten.
Common APIs clearly are favored by both the open source community and the
bioinformatics community for these reasons.

\subsubsection{Sharing}

%@ sharing -> web services
Related to the concept of open APIs for interacting with biological data is the
concept of web services.  Sharing of biological data, whether it is raw,
primary data, or processed results, is of the utmost concern for biologist.
Yet journal articles provide a poor repository for large datasets.  Simply
downloading large data files is a better approach, yet many times only a few
pieces of information are needed and the downloading of an entire dataset is
unnecessary.  For example, if a researcher wants a particular protein structure
from the protein data bank (PDB, \url{http://www.pdb.org}) downloading the
thousands of structures in bulk from the PDB is unnecessary.

The emerging web services approach used across the Internet points to a better
solution.  This model includes technology such as the simple object access
protocol and REST concepts for interacting with an API remotely over the
hypertext transport protocol (HTTP) on which the web is based.  This idea is
that simple requests for data or information calculated on the fly can be made
by a researcher and the result is calculated or retrieved remotely and returned
over the Internet.  This type of approach is extremely flexible and can be used
in a variety of contexts to present biological data to other researchers.

The distributed annotation system, or DAS, is a popular bioinformatics web
services project geared towards the sharing of genome annotations with the
larger research community \cite{dowell2004das}.  Organizations looking to share
genome assemblies, gene annotations, and other genomic features use DAS to make
this information available over the web.  Implementations of DAS use the
standard HTTP protocol and XML as an exchange standard.  The next version,
DAS/2, expands on the genomics focus of DAS by including capabilities to
exchange ontologies, download experimental assay results such as microarray
data, and perform on-demand sequence analysis such as BLAST.  The success of
DAS as a project is due to the ease of which scientists can utilize information
published with DAS.  Many clients exist, such as GBrowse and IGB, and the web
services model affords programmatic access to the servers \cite{stein2002ggb}.
This allows additional applications to be built on top of these public
repositories.  For example, the Celsius project web interfaces were created
on top of a DAS/2 server which provided the raw data.  These web tools let an
end user query the microarray data available via ontology annotations and
download the corresponding data in a variety of different processed forms.

More information on web services can be found in Chapters \XXXchapter,
\gmodwebchapter, and the Appendix. Chapter \XXXchapter\ details
the creation of a web services designed to thread protein sequences onto know
structures with the end goal of identifying disulfide bonds.  Chapter
\gmodwebchapter\ examines a model organism website generation framework that
includes web services tools.

\subsubsection{Standardization}

%@ standardization -> software packaging
A closely related concept to web services is the concept of software
standardization.  In the model of web services, a researcher can focus on the
analysis of data and its biological meaning rather than figuring out how to
store data locally.  This approach affords abstraction between the researcher
and the entity providing the web service, making it easier for others to
either validate existing work by performing the same analysis or expand on the
work using the same web services.  It allows researchers to easily standardize
a given dataset or analysis server.  Another technique familiar to all
computer users on standardization is the versioning of computer programs.  When
research is being performed on a particular dataset or with a particular
software program, it is extremely important to track which version was
used.  Otherwise it becomes impossible to replicate the work.  The idea of
software packaging, borrowed from the field of information technology, is of
key importance to bioinformatics.  In addition to simply versioning software,
many comprehensive systems exist for specifically tracking, installing, and
updating both software and data in a particular computing environment.  The
Linux system, for example, uses one of several different package managers to
perform this task.

The Biopackages project looks to standardize many tools used commonly in
bioinformatics projects. It encompasses an automated build system that creates
software packages for particular Linux distributions.  These include packages
for APIs such as BioPerl and BioConductor, web services such as DAS/2, and
databases such as Chado.  Details of the construction, public availability, and
benefits of this standardization tool can be found in Chapter \biopackageschapter.

%@ I have worked on several software solutions for these (maybe just integrate
%these above) This can just be a "Chapter X shows..." paragraph that maps the
%topics above to chapters. I'm just going to include these above.

\subsection{Further Work}

%@ Future biologies will be driven by basic needs as biology continues to
%become integrated with other disciplines.
Biology will continue to be driven by the basic needs to store, annotate,
analyze, share, and standardize biological data and practices. As biology
continues to become more integrated with other disciplines and influences the
development of other fields at the same time, new and effective technologies
will continue to be developed. In this section, many facets of computer science
and information technology were explored as they relate to the common problems
facing biologists today and in the future.  The use of computer science and
information technology continues to expand in biological research and provides
new avenues to address challenges and demands of this transforming field.
